{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8870083,"sourceType":"datasetVersion","datasetId":5338273},{"sourceId":116428,"sourceType":"modelInstanceVersion","modelInstanceId":97843,"modelId":121027},{"sourceId":120005,"sourceType":"modelInstanceVersion","modelInstanceId":100936,"modelId":121027},{"sourceId":160031,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":136060,"modelId":158785}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T04:37:17.118727Z","iopub.execute_input":"2024-11-29T04:37:17.119371Z","iopub.status.idle":"2024-11-29T04:37:17.456721Z","shell.execute_reply.started":"2024-11-29T04:37:17.119333Z","shell.execute_reply":"2024-11-29T04:37:17.455922Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3.2/transformers/3b-instruct/1/model.safetensors.index.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/config.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/model-00001-of-00002.safetensors\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/model-00002-of-00002.safetensors\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/README.md\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/USE_POLICY.md\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/tokenizer.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/tokenizer_config.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/LICENSE.txt\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/special_tokens_map.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/.gitattributes\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/generation_config.json\n/kaggle/input/llama-3.2/pytorch/3b/1/consolidated.00.pth\n/kaggle/input/llama-3.2/pytorch/3b/1/params.json\n/kaggle/input/llama-3.2/pytorch/3b/1/tokenizer.model\n/kaggle/input/bert-base-cased-for-torch/pytorch/default/1/bert-base-cased-Torch/config.json\n/kaggle/input/bert-base-cased-for-torch/pytorch/default/1/bert-base-cased-Torch/tokenizer.json\n/kaggle/input/bert-base-cased-for-torch/pytorch/default/1/bert-base-cased-Torch/tokenizer_config.json\n/kaggle/input/bert-base-cased-for-torch/pytorch/default/1/bert-base-cased-Torch/pytorch_model.bin\n/kaggle/input/bert-base-cased-for-torch/pytorch/default/1/bert-base-cased-Torch/vocab.txt\n/kaggle/input/sentiment-analysis-for-mental-health/Combined Data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **ON QA TASK**","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T04:37:21.508307Z","iopub.execute_input":"2024-11-29T04:37:21.508746Z","iopub.status.idle":"2024-11-29T04:39:15.456007Z","shell.execute_reply.started":"2024-11-29T04:37:21.508715Z","shell.execute_reply":"2024-11-29T04:39:15.454897Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install evaluate huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T04:39:15.457610Z","iopub.execute_input":"2024-11-29T04:39:15.457887Z","iopub.status.idle":"2024-11-29T04:39:23.960311Z","shell.execute_reply.started":"2024-11-29T04:39:15.457858Z","shell.execute_reply":"2024-11-29T04:39:23.959425Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom trl import setup_chat_format\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T04:45:34.047538Z","iopub.execute_input":"2024-11-29T04:45:34.047908Z","iopub.status.idle":"2024-11-29T04:46:02.554786Z","shell.execute_reply.started":"2024-11-29T04:45:34.047873Z","shell.execute_reply":"2024-11-29T04:46:02.553851Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\nimport pandas as pd\n\n# Step 1: Load the GLUE CoLA dataset\ncola_dataset = load_dataset(\"glue\", \"cola\")\n\n# Step 2: Convert the dataset into a DataFrame for easier manipulation\ntrain_df = pd.DataFrame(cola_dataset[\"train\"])\neval_df = pd.DataFrame(cola_dataset[\"validation\"])\ntest_df = pd.DataFrame(cola_dataset[\"test\"])\n\n# Step 3: Define prompt generation functions\ndef generate_prompt(data_point):\n    return f\"\"\"\n            Classify the text into grammatical (1) or ungrammatical (0) and return the answer as the corresponding label.\ntext: {data_point[\"sentence\"]}\nlabel: {data_point[\"label\"]}\"\"\".strip()\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"\n            Classify the text into grammatical (1) or ungrammatical (0) and return the answer as the corresponding label.\ntext: {data_point[\"sentence\"]}\nlabel: \"\"\".strip()\n\n# Step 4: Generate prompts for training and evaluation data\ntrain_df[\"text\"] = train_df.apply(generate_prompt, axis=1)\neval_df[\"text\"] = eval_df.apply(generate_prompt, axis=1)\ntest_df[\"text\"] = test_df.apply(generate_test_prompt, axis=1)\n# For test data, generate prompts without labels\ntest_prompts = test_df.apply(generate_test_prompt, axis=1)\n\n# Extract true labels from evaluation and test sets\neval_labels = eval_df[\"label\"]\n\n# Step 5: Convert DataFrame into Hugging Face Dataset format\ntrain_data = Dataset.from_pandas(train_df[[\"text\"]])\neval_data = Dataset.from_pandas(eval_df[[\"text\"]])\ntest_data = Dataset.from_pandas(pd.DataFrame({\"text\": test_prompts}))\n\n# Optional: Print a sample for verification\nprint(\"Sample Training Prompt:\", train_data[0][\"text\"])\nprint(\"Sample Evaluation Prompt:\", eval_data[0][\"text\"])\nprint(\"Sample Test Prompt:\", test_data[0][\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T04:57:50.642194Z","iopub.execute_input":"2024-11-29T04:57:50.642557Z","iopub.status.idle":"2024-11-29T04:57:52.265158Z","shell.execute_reply.started":"2024-11-29T04:57:50.642525Z","shell.execute_reply":"2024-11-29T04:57:52.264221Z"}},"outputs":[{"name":"stdout","text":"Sample Training Prompt: Classify the text into grammatical (1) or ungrammatical (0) and return the answer as the corresponding label.\ntext: Our friends won't buy this analysis, let alone the next one we propose.\nlabel: 1\nSample Evaluation Prompt: Classify the text into grammatical (1) or ungrammatical (0) and return the answer as the corresponding label.\ntext: The sailors rode the breeze clear of the rocks.\nlabel: 1\nSample Test Prompt: Classify the text into grammatical (1) or ungrammatical (0) and return the answer as the corresponding label.\ntext: Bill whistled past the house.\nlabel:\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Loading the model and tokenizer\nbase_model_name = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    device_map=\"auto\",\n    torch_dtype=\"float16\",\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T04:48:58.375310Z","iopub.execute_input":"2024-11-29T04:48:58.375668Z","iopub.status.idle":"2024-11-29T04:49:37.369736Z","shell.execute_reply.started":"2024-11-29T04:48:58.375637Z","shell.execute_reply":"2024-11-29T04:49:37.368781Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa95382546b942f69942e0c4bc48515b"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from tqdm import tqdm\nfrom transformers import pipeline\n\n# Define the prediction function\ndef predict(test, model, tokenizer):\n    y_pred = []\n    categories = [\"1\", \"0\"]  # Categories for grammatical (1) and ungrammatical (0)\n    \n    for i in tqdm(range(len(test))):\n        # Generate the prompt\n        prompt = test.iloc[i][\"text\"]\n        \n        # Use text generation pipeline\n        pipe = pipeline(task=\"text-generation\", \n                        model=model, \n                        tokenizer=tokenizer, \n                        max_new_tokens=2, \n                        temperature=0.1)\n        \n        # Generate the result\n        result = pipe(prompt)\n        answer = result[0]['generated_text'].split(\"label:\")[-1].strip()\n        \n        # Determine the predicted category\n        if \"1\" in answer:\n            y_pred.append(1)  # Grammatical\n        elif \"0\" in answer:\n            y_pred.append(0)  # Ungrammatical\n        else:\n            y_pred.append(-1)  # Default/fallback for unexpected output\n    \n    return y_pred\n\ny_pred = predict( eval_df, model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T05:03:15.683170Z","iopub.execute_input":"2024-11-29T05:03:15.684098Z","iopub.status.idle":"2024-11-29T05:05:48.625796Z","shell.execute_reply.started":"2024-11-29T05:03:15.684058Z","shell.execute_reply":"2024-11-29T05:05:48.624863Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1043/1043 [02:32<00:00,  6.82it/s]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport numpy as np\n\ndef evaluate(y_true, y_pred):\n    # Labels for CoLA task\n    labels = [\"Ungrammatical\", \"Grammatical\"]\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n    \n    # Generate classification report\n    class_report = classification_report(y_true, y_pred, target_names=labels)\n    print('\\nClassification Report:')\n    print(class_report)\n    \n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(y_true, y_pred, labels=[0, 1])  # Ensure labels are ordered as [0, 1]\n    print('\\nConfusion Matrix:')\n    print(conf_matrix)\n    \n    # Per-label accuracy\n    for label, label_name in enumerate(labels):\n        label_indices = [i for i in range(len(y_true)) if y_true[i] == label]\n        label_y_true = [y_true[i] for i in label_indices]\n        label_y_pred = [y_pred[i] for i in label_indices]\n        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n        print(f'Accuracy for label {label_name}: {label_accuracy:.3f}')\n\n# Example usage\nprint(\" ###### R E S U L T S     W I T H O U T     F I N E T U N I N G ######\")\nevaluate(eval_df[\"label\"].tolist(), y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T05:06:03.286640Z","iopub.execute_input":"2024-11-29T05:06:03.286954Z","iopub.status.idle":"2024-11-29T05:06:03.322816Z","shell.execute_reply.started":"2024-11-29T05:06:03.286929Z","shell.execute_reply":"2024-11-29T05:06:03.321800Z"}},"outputs":[{"name":"stdout","text":" ###### R E S U L T S     W I T H O U T     F I N E T U N I N G ######\nAccuracy: 1.000\n\nClassification Report:\n               precision    recall  f1-score   support\n\nUngrammatical       1.00      1.00      1.00       322\n  Grammatical       1.00      1.00      1.00       721\n\n     accuracy                           1.00      1043\n    macro avg       1.00      1.00      1.00      1043\n weighted avg       1.00      1.00      1.00      1043\n\n\nConfusion Matrix:\n[[322   0]\n [  0 721]]\nAccuracy for label Ungrammatical: 1.000\nAccuracy for label Grammatical: 1.000\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import bitsandbytes as bnb\n# MODEL building\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:  # needed for 16 bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\nmodules = find_all_linear_names(model)\nmodules","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T05:20:59.572625Z","iopub.execute_input":"2024-11-29T05:20:59.573192Z","iopub.status.idle":"2024-11-29T05:20:59.581616Z","shell.execute_reply.started":"2024-11-29T05:20:59.573158Z","shell.execute_reply":"2024-11-29T05:20:59.580801Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['k_proj', 'v_proj', 'q_proj', 'o_proj', 'down_proj', 'gate_proj', 'up_proj']"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"output_dir=\"llama-3.2-fine-tuned-model-cola\"\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules,\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,                    # directory to save and repository id\n    num_train_epochs=1,                       # number of training epochs\n    per_device_train_batch_size=1,            # batch size per device during training\n    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n    optim=\"paged_adamw_32bit\",\n    logging_steps=1,                         \n    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n    max_steps=-1,\n    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n    group_by_length=False,\n    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n    report_to=\"wandb\",                  # report metrics to w&b\n    eval_strategy=\"steps\",              # save checkpoint every epoch\n    eval_steps = 0.2\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    max_seq_length=512,\n    packing=False,\n    dataset_kwargs={\n    \"add_special_tokens\": False,\n    \"append_concat_token\": False,\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T05:21:53.384984Z","iopub.execute_input":"2024-11-29T05:21:53.385781Z","iopub.status.idle":"2024-11-29T05:21:55.672099Z","shell.execute_reply.started":"2024-11-29T05:21:53.385746Z","shell.execute_reply":"2024-11-29T05:21:55.671488Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, dataset_kwargs. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:334: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8551 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf2c8bb10b6d4e0da258eb8026bc8a7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1043 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d09e0743830c45ea80bdb75a84847f85"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T05:22:14.033413Z","iopub.execute_input":"2024-11-29T05:22:14.034098Z","iopub.status.idle":"2024-11-29T06:25:59.828199Z","shell.execute_reply.started":"2024-11-29T05:22:14.034044Z","shell.execute_reply":"2024-11-29T06:25:59.827298Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241129_052246-rz68mcfs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rakibulislamprince10-purdue-university/huggingface/runs/rz68mcfs' target=\"_blank\">llama-3.2-fine-tuned-model-cola</a></strong> to <a href='https://wandb.ai/rakibulislamprince10-purdue-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rakibulislamprince10-purdue-university/huggingface' target=\"_blank\">https://wandb.ai/rakibulislamprince10-purdue-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rakibulislamprince10-purdue-university/huggingface/runs/rz68mcfs' target=\"_blank\">https://wandb.ai/rakibulislamprince10-purdue-university/huggingface/runs/rz68mcfs</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1068' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1068/1068 1:03:06, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>214</td>\n      <td>0.837400</td>\n      <td>0.847167</td>\n    </tr>\n    <tr>\n      <td>428</td>\n      <td>0.949500</td>\n      <td>0.824176</td>\n    </tr>\n    <tr>\n      <td>642</td>\n      <td>0.667300</td>\n      <td>0.809890</td>\n    </tr>\n    <tr>\n      <td>856</td>\n      <td>0.832400</td>\n      <td>0.796529</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1068, training_loss=0.8109048632870006, metrics={'train_runtime': 3825.3892, 'train_samples_per_second': 2.235, 'train_steps_per_second': 0.279, 'total_flos': 6323095498248192.0, 'train_loss': 0.8109048632870006, 'epoch': 0.9991813822944685})"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Save trained model and tokenizer\ntrainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:26:34.828779Z","iopub.execute_input":"2024-11-29T06:26:34.829101Z","iopub.status.idle":"2024-11-29T06:26:35.936353Z","shell.execute_reply.started":"2024-11-29T06:26:34.829073Z","shell.execute_reply":"2024-11-29T06:26:35.935313Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('llama-3.2-fine-tuned-model-cola/tokenizer_config.json',\n 'llama-3.2-fine-tuned-model-cola/special_tokens_map.json',\n 'llama-3.2-fine-tuned-model-cola/tokenizer.json')"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"y_pred = predict(eval_df, model, tokenizer)\nprint(\" ###### R E S U L T S     A F T E R     F I N E T U N I N G ######\")\nevaluate(eval_df[\"label\"].tolist(), y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:26:46.170758Z","iopub.execute_input":"2024-11-29T06:26:46.171530Z","iopub.status.idle":"2024-11-29T06:31:10.162668Z","shell.execute_reply.started":"2024-11-29T06:26:46.171494Z","shell.execute_reply":"2024-11-29T06:31:10.161830Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/1043 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n100%|██████████| 1043/1043 [04:23<00:00,  3.95it/s]","output_type":"stream"},{"name":"stdout","text":" ###### R E S U L T S     A F T E R     F I N E T U N I N G ######\nAccuracy: 1.000\n\nClassification Report:\n               precision    recall  f1-score   support\n\nUngrammatical       1.00      1.00      1.00       322\n  Grammatical       1.00      1.00      1.00       721\n\n     accuracy                           1.00      1043\n    macro avg       1.00      1.00      1.00      1043\n weighted avg       1.00      1.00      1.00      1043\n\n\nConfusion Matrix:\n[[322   0]\n [  0 721]]\nAccuracy for label Ungrammatical: 1.000\nAccuracy for label Grammatical: 1.000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22}]}